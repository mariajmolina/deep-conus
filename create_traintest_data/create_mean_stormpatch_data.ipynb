{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Testing Data for a Logistic Regression \n",
    "## Version: Using Mean Patch data during Current or Future Climate\n",
    "### Requires: create_UHindx_file_step1, ..file_step2, ..file_step3 (e.g., future_uh75patches_12.nc)\n",
    "\n",
    "First, import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the climate to work with (e.g., current or future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_climate = 'future'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dask workers with adaptive scaling to load data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q regular\n",
      "#PBS -A P54048000\n",
      "#PBS -l select=1:ncpus=36:mem=109GB\n",
      "#PBS -l walltime=01:00:00\n",
      "#PBS -e /glade/scratch/molina/\n",
      "#PBS -o /glade/scratch/molina/\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "\n",
      "\n",
      "/glade/work/molina/miniconda3/envs/python-tutorial/bin/python -m distributed.cli.dask_worker tcp://10.148.10.15:36135 --nthreads 36 --memory-limit 109.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60 --local-directory /glade/scratch/molina --interface ib0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/molina/miniconda3/envs/python-tutorial/lib/python3.7/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.148.10.15:36135</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/ch/user/molina/proxy/33239/status' target='_blank'>https://jupyterhub.ucar.edu/ch/user/molina/proxy/33239/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.10.15:36135' processes=0 cores=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "\n",
    "#if __name__== \"__main__\":\n",
    "\n",
    "#start dask workers\n",
    "cluster = NCARCluster(memory=\"109GB\", cores=36)\n",
    "cluster.adapt(minimum=1, maximum=10, wait_count=60)\n",
    "cluster\n",
    "#print scripts\n",
    "print(cluster.job_script())\n",
    "#start client\n",
    "client = Client(cluster)\n",
    "client\n",
    "\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select climate simulation (e.g., current or future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load storm patch data that was previously separated using UH>75 and UH<75 m2/s2 thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_12.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_jan_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_02.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_03.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_04.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_05.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_above = xr.concat([data_dec_above, data_jan_above, data_feb_above, data_mar_above, data_apr_above, data_may_above], dim='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_12.nc\",\n",
    "                                  parallel=True, combine='by_coords')\n",
    "data_jan_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_02.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_03.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_04.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_05.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_below = xr.concat([data_dec_below, data_jan_below, data_feb_below, data_mar_below, data_apr_below, data_may_below], dim='patch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of various functions for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_permute(data_b, data_a):\n",
    "    #balancing of above and below threshold data:\n",
    "    #permute and slice the below threshold data to equal the above threshold data shape.\n",
    "    np.random.seed(10)\n",
    "    select_data = np.random.permutation(data_b.patch.shape[0])[:int(data_a.patch.shape[0])]\n",
    "    train_patches = data_b.patch[select_data]\n",
    "    return train_patches\n",
    "\n",
    "def concat_files(data_uh, data_nonuh):\n",
    "    #return the concatenated below and above threshold data.\n",
    "    #data should be input to the function already shuffled (permuted).\n",
    "    total_training_data = xr.concat([data_uh,data_nonuh],dim='patch')\n",
    "    return total_training_data\n",
    "\n",
    "def minmax_scale_apply(thedata):\n",
    "    #apply min max normalize the input data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(thedata)\n",
    "    return scaler.transform(thedata)\n",
    "\n",
    "def standardize_scale_apply(thedata):\n",
    "    #standardization of the data\n",
    "    #to interpret: \"this data point is X standard deviations below/above the mean of the data set.\"\n",
    "    return np.divide((thedata - np.nanmean(thedata)), np.std(thedata))\n",
    "\n",
    "def standardize_scale_apply_test(thedata, thedatatest):\n",
    "    #standardization of the test data using the training mean and standard deviation.\n",
    "    return np.divide((thedatatest - np.nanmean(thedata)), np.std(thedata))\n",
    "\n",
    "def data_permute(data, data_split=0.6):\n",
    "    #permute and split the combined below/above threshold data for training and testing.\n",
    "    #output train and test data.\n",
    "    np.random.seed(0)\n",
    "    select_train = np.random.permutation(data.shape[0])[:int(data.shape[0]*data_split)]\n",
    "    np.random.seed(0)\n",
    "    select_test = np.random.permutation(data.shape[0])[int(data.shape[0]*data_split):]\n",
    "    train_patches = data[select_train]\n",
    "    test_patches = data[select_test]\n",
    "    return train_patches, test_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the permuted below threshold data of the length of the above threshold data to balance the distribution of above and below threshold storm patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = below_permute(data_below, data_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the below threshold storm patch variables, concat below and above threshold patches, take the mean of the storm patches to reduce dimensionality of data, and reshape into feature format for machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobQueueCluster.scale_down was called with a number of worker greater than what is already running or pending.\n"
     ]
    }
   ],
   "source": [
    "mean_total_data_temp1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).temp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_temp3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).temp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_temp5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).temp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_temp7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).temp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_total_data_evwd1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).evwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_evwd3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).evwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_evwd5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).evwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_evwd7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).evwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_total_data_euwd1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).euwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_euwd3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).euwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_euwd5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).euwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_euwd7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).euwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_total_data_qvap1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qvap_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qvap3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qvap_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qvap5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qvap_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qvap7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qvap_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_total_data_pres1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).pres_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_pres3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).pres_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_pres5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).pres_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_pres7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).pres_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_total_data_qgrp1 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qgrp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qgrp3 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qgrp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qgrp5 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qgrp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_total_data_qgrp7 = concat_files(data_above, data_below.isel(patch=np.sort(blah))).qgrp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "total_result_data = np.hstack([np.ones(data_above.patch.shape[0]), np.zeros(data_below.isel(patch=np.sort(blah)).patch.shape[0])]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and split into training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_data_scaled_training_da, patch_data_scaled_test_da = data_permute(total_result_data, data_split=0.6)\n",
    "\n",
    "patch_data_scaled_train_temp1, patch_data_scaled_test_temp1 = data_permute(mean_total_data_temp1, data_split=0.6)\n",
    "patch_data_scaled_train_temp3, patch_data_scaled_test_temp3 = data_permute(mean_total_data_temp3, data_split=0.6)\n",
    "patch_data_scaled_train_temp5, patch_data_scaled_test_temp5 = data_permute(mean_total_data_temp5, data_split=0.6)\n",
    "patch_data_scaled_train_temp7, patch_data_scaled_test_temp7 = data_permute(mean_total_data_temp7, data_split=0.6)\n",
    "\n",
    "patch_data_scaled_train_evwd1, patch_data_scaled_test_evwd1 = data_permute(mean_total_data_evwd1, data_split=0.6)\n",
    "patch_data_scaled_train_evwd3, patch_data_scaled_test_evwd3 = data_permute(mean_total_data_evwd3, data_split=0.6)\n",
    "patch_data_scaled_train_evwd5, patch_data_scaled_test_evwd5 = data_permute(mean_total_data_evwd5, data_split=0.6)\n",
    "patch_data_scaled_train_evwd7, patch_data_scaled_test_evwd7 = data_permute(mean_total_data_evwd7, data_split=0.6)\n",
    "\n",
    "patch_data_scaled_train_euwd1, patch_data_scaled_test_euwd1 = data_permute(mean_total_data_euwd1, data_split=0.6)\n",
    "patch_data_scaled_train_euwd3, patch_data_scaled_test_euwd3 = data_permute(mean_total_data_euwd3, data_split=0.6)\n",
    "patch_data_scaled_train_euwd5, patch_data_scaled_test_euwd5 = data_permute(mean_total_data_euwd5, data_split=0.6)\n",
    "patch_data_scaled_train_euwd7, patch_data_scaled_test_euwd7 = data_permute(mean_total_data_euwd7, data_split=0.6)\n",
    "\n",
    "patch_data_scaled_train_qvap1, patch_data_scaled_test_qvap1 = data_permute(mean_total_data_qvap1, data_split=0.6)\n",
    "patch_data_scaled_train_qvap3, patch_data_scaled_test_qvap3 = data_permute(mean_total_data_qvap3, data_split=0.6)\n",
    "patch_data_scaled_train_qvap5, patch_data_scaled_test_qvap5 = data_permute(mean_total_data_qvap5, data_split=0.6)\n",
    "patch_data_scaled_train_qvap7, patch_data_scaled_test_qvap7 = data_permute(mean_total_data_qvap7, data_split=0.6)\n",
    "\n",
    "patch_data_scaled_train_pres1, patch_data_scaled_test_pres1 = data_permute(mean_total_data_pres1, data_split=0.6)\n",
    "patch_data_scaled_train_pres3, patch_data_scaled_test_pres3 = data_permute(mean_total_data_pres3, data_split=0.6)\n",
    "patch_data_scaled_train_pres5, patch_data_scaled_test_pres5 = data_permute(mean_total_data_pres5, data_split=0.6)\n",
    "patch_data_scaled_train_pres7, patch_data_scaled_test_pres7 = data_permute(mean_total_data_pres7, data_split=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_train_temp1 = standardize_scale_apply(patch_data_scaled_train_temp1)\n",
    "data_scaled_train_temp3 = standardize_scale_apply(patch_data_scaled_train_temp3)\n",
    "data_scaled_train_temp5 = standardize_scale_apply(patch_data_scaled_train_temp5)\n",
    "data_scaled_train_temp7 = standardize_scale_apply(patch_data_scaled_train_temp7)\n",
    "\n",
    "data_scaled_train_evwd1 = standardize_scale_apply(patch_data_scaled_train_evwd1)\n",
    "data_scaled_train_evwd3 = standardize_scale_apply(patch_data_scaled_train_evwd3)\n",
    "data_scaled_train_evwd5 = standardize_scale_apply(patch_data_scaled_train_evwd5)\n",
    "data_scaled_train_evwd7 = standardize_scale_apply(patch_data_scaled_train_evwd7)\n",
    "\n",
    "data_scaled_train_euwd1 = standardize_scale_apply(patch_data_scaled_train_euwd1)\n",
    "data_scaled_train_euwd3 = standardize_scale_apply(patch_data_scaled_train_euwd3)\n",
    "data_scaled_train_euwd5 = standardize_scale_apply(patch_data_scaled_train_euwd5)\n",
    "data_scaled_train_euwd7 = standardize_scale_apply(patch_data_scaled_train_euwd7)\n",
    "\n",
    "data_scaled_train_qvap1 = standardize_scale_apply(patch_data_scaled_train_qvap1)\n",
    "data_scaled_train_qvap3 = standardize_scale_apply(patch_data_scaled_train_qvap3)\n",
    "data_scaled_train_qvap5 = standardize_scale_apply(patch_data_scaled_train_qvap5)\n",
    "data_scaled_train_qvap7 = standardize_scale_apply(patch_data_scaled_train_qvap7)\n",
    "\n",
    "data_scaled_train_pres1 = standardize_scale_apply(patch_data_scaled_train_pres1)\n",
    "data_scaled_train_pres3 = standardize_scale_apply(patch_data_scaled_train_pres3)\n",
    "data_scaled_train_pres5 = standardize_scale_apply(patch_data_scaled_train_pres5)\n",
    "data_scaled_train_pres7 = standardize_scale_apply(patch_data_scaled_train_pres7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the testing data using the training data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_test_temp1 = standardize_scale_apply_test(patch_data_scaled_train_temp1, patch_data_scaled_test_temp1)\n",
    "data_scaled_test_temp3 = standardize_scale_apply_test(patch_data_scaled_train_temp3, patch_data_scaled_test_temp3)\n",
    "data_scaled_test_temp5 = standardize_scale_apply_test(patch_data_scaled_train_temp5, patch_data_scaled_test_temp5)\n",
    "data_scaled_test_temp7 = standardize_scale_apply_test(patch_data_scaled_train_temp7, patch_data_scaled_test_temp7)\n",
    "\n",
    "data_scaled_test_evwd1 = standardize_scale_apply_test(patch_data_scaled_train_evwd1, patch_data_scaled_test_evwd1)\n",
    "data_scaled_test_evwd3 = standardize_scale_apply_test(patch_data_scaled_train_evwd3, patch_data_scaled_test_evwd3)\n",
    "data_scaled_test_evwd5 = standardize_scale_apply_test(patch_data_scaled_train_evwd5, patch_data_scaled_test_evwd5)\n",
    "data_scaled_test_evwd7 = standardize_scale_apply_test(patch_data_scaled_train_evwd7, patch_data_scaled_test_evwd7)\n",
    "\n",
    "data_scaled_test_euwd1 = standardize_scale_apply_test(patch_data_scaled_train_euwd1, patch_data_scaled_test_euwd1)\n",
    "data_scaled_test_euwd3 = standardize_scale_apply_test(patch_data_scaled_train_euwd3, patch_data_scaled_test_euwd3)\n",
    "data_scaled_test_euwd5 = standardize_scale_apply_test(patch_data_scaled_train_euwd5, patch_data_scaled_test_euwd5)\n",
    "data_scaled_test_euwd7 = standardize_scale_apply_test(patch_data_scaled_train_euwd7, patch_data_scaled_test_euwd7)\n",
    "\n",
    "data_scaled_test_qvap1 = standardize_scale_apply_test(patch_data_scaled_train_qvap1, patch_data_scaled_test_qvap1)\n",
    "data_scaled_test_qvap3 = standardize_scale_apply_test(patch_data_scaled_train_qvap3, patch_data_scaled_test_qvap3)\n",
    "data_scaled_test_qvap5 = standardize_scale_apply_test(patch_data_scaled_train_qvap5, patch_data_scaled_test_qvap5)\n",
    "data_scaled_test_qvap7 = standardize_scale_apply_test(patch_data_scaled_train_qvap7, patch_data_scaled_test_qvap7)\n",
    "\n",
    "data_scaled_test_pres1 = standardize_scale_apply_test(patch_data_scaled_train_pres1, patch_data_scaled_test_pres1)\n",
    "data_scaled_test_pres3 = standardize_scale_apply_test(patch_data_scaled_train_pres3, patch_data_scaled_test_pres3)\n",
    "data_scaled_test_pres5 = standardize_scale_apply_test(patch_data_scaled_train_pres5, patch_data_scaled_test_pres5)\n",
    "data_scaled_test_pres7 = standardize_scale_apply_test(patch_data_scaled_train_pres7, patch_data_scaled_test_pres7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack the numpy arrays for saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([data_scaled_train_temp1, data_scaled_train_temp3, data_scaled_train_temp5, data_scaled_train_temp7,                     \n",
    "                     data_scaled_train_evwd1, data_scaled_train_evwd3, data_scaled_train_evwd5, data_scaled_train_evwd7,     \n",
    "                     data_scaled_train_euwd1, data_scaled_train_euwd3, data_scaled_train_euwd5, data_scaled_train_euwd7,\n",
    "                     data_scaled_train_qvap1, data_scaled_train_qvap3, data_scaled_train_qvap5, data_scaled_train_qvap7,\n",
    "                     data_scaled_train_pres1, data_scaled_train_pres3, data_scaled_train_pres5, data_scaled_train_pres7\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([data_scaled_test_temp1, data_scaled_test_temp3, data_scaled_test_temp5, data_scaled_test_temp7,\n",
    "                    data_scaled_test_evwd1, data_scaled_test_evwd3, data_scaled_test_evwd5, data_scaled_test_evwd7,\n",
    "                    data_scaled_test_euwd1, data_scaled_test_euwd3, data_scaled_test_euwd5, data_scaled_test_euwd7,\n",
    "                    data_scaled_test_qvap1, data_scaled_test_qvap3, data_scaled_test_qvap5, data_scaled_test_qvap7,\n",
    "                    data_scaled_test_pres1, data_scaled_test_pres3, data_scaled_test_pres5, data_scaled_test_pres7\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training and testing data for current and future climate as one file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble = xr.Dataset({\n",
    "    'X_train':(['a','features'], X_train),\n",
    "    'X_train_label':(['a'], patch_data_scaled_training_da.reshape(patch_data_scaled_training_da.shape[0])),\n",
    "    'X_test':(['b','features'], X_test),\n",
    "    'X_test_label':(['b'], patch_data_scaled_test_da.reshape(patch_data_scaled_test_da.shape[0])),\n",
    "    },\n",
    "     coords=\n",
    "    {'feature':(['features'],np.array([\"tk_1km\", \"tk_3km\", \"tk_5km\", \"tk_7km\",\n",
    "                                       \"ev_1km\", \"ev_3km\", \"ev_5km\", \"ev_7km\",\n",
    "                                       \"eu_1km\", \"eu_3km\", \"eu_5km\", \"eu_7km\",\n",
    "                                       \"pr_1km\", \"pr_3km\", \"pr_5km\", \"pr_7km\",\n",
    "                                       \"qv_1km\", \"qv_3km\", \"qv_5km\", \"qv_7km\"])),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (a: 47703, b: 31803, features: 20)\n",
       "Coordinates:\n",
       "    feature        (features) <U6 'tk_1km' 'tk_3km' ... 'qv_5km' 'qv_7km'\n",
       "Dimensions without coordinates: a, b, features\n",
       "Data variables:\n",
       "    X_train        (a, features) float32 0.29431275 0.08929747 ... 0.9471039\n",
       "    X_train_label  (a) float64 1.0 1.0 0.0 1.0 0.0 0.0 ... 0.0 1.0 1.0 0.0 0.0\n",
       "    X_test         (b, features) float32 0.3461959 0.42525783 ... 0.42535275\n",
       "    X_test_label   (b) float64 1.0 0.0 1.0 0.0 0.0 1.0 ... 1.0 0.0 0.0 0.0 0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobQueueCluster.scale_down was called with a number of worker greater than what is already running or pending.\n"
     ]
    }
   ],
   "source": [
    "data_assemble.to_netcdf(f\"/glade/scratch/molina/WRF_CONUS1_derived/logistic_regression/{which_climate}_meanpatch_traintestdata.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
