{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Testing Data for a Logistic Regression\n",
    "## Version: Using Mean Patch data during Current or Future Climate (UNBALANCED)\n",
    "### Requires: create_UHindx_file_step1, ..file_step2, ..file_step3 (e.g., future_uh75patches_12.nc)\n",
    "First, import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the climate to work with (e.g., current or future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_climate = 'current'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dask workers with adaptive scaling to load data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q regular\n",
      "#PBS -A P54048000\n",
      "#PBS -l select=1:ncpus=36:mem=109GB\n",
      "#PBS -l walltime=01:00:00\n",
      "#PBS -e /glade/scratch/molina/\n",
      "#PBS -o /glade/scratch/molina/\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "\n",
      "\n",
      "/glade/work/molina/miniconda3/envs/python-tutorial/bin/python -m distributed.cli.dask_worker tcp://10.148.10.17:36175 --nthreads 36 --memory-limit 109.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60 --local-directory /glade/scratch/molina --interface ib0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.148.10.17:36175</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status' target='_blank'>https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.10.17:36175' processes=0 cores=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "\n",
    "#if __name__== \"__main__\":\n",
    "\n",
    "#start dask workers\n",
    "cluster = NCARCluster(memory=\"109GB\", cores=36)\n",
    "cluster.adapt(minimum=1, maximum=10, wait_count=60)\n",
    "cluster\n",
    "#print scripts\n",
    "print(cluster.job_script())\n",
    "#start client\n",
    "client = Client(cluster)\n",
    "client\n",
    "\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load storm patch data that was previously separated using UH>75 and UH<75 m2/s2 thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_12.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_jan_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_02.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_03.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_04.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_05.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_above = xr.concat([data_dec_above, data_jan_above, data_feb_above, data_mar_above, data_apr_above, data_may_above], dim='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_12.nc\",\n",
    "                                  parallel=True, combine='by_coords')\n",
    "data_jan_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_02.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_03.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_04.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_05.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_below = xr.concat([data_dec_below, data_jan_below, data_feb_below, data_mar_below, data_apr_below, data_may_below], dim='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of strongly rotating to not strongly rotating storm patches expressed as a percent is about:  2 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The ratio of strongly rotating to not strongly rotating storm patches expressed as a percent is about: \", \n",
    "      round((data_above.patch.size/data_below.patch.size)*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of various functions for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traintest_data(data_b, data_a, split_perc=0.6, return_label=False):\n",
    "    #balancing of above and below threshold data for training data, spitting out remainder for testing\n",
    "    #permute and slice the below threshold data to equal the above threshold data shape.\n",
    "    \n",
    "    #train above\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_a.shape[0])[:int(data_a.shape[0]*split_perc)]\n",
    "    train_above = data_a[select_data]\n",
    "    \n",
    "    #train below\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_b.shape[0])[:int(data_a.shape[0]*split_perc)]\n",
    "    train_below = data_b[select_data]\n",
    "    \n",
    "    #test above\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_a.shape[0])[int(data_a.shape[0]*split_perc):]\n",
    "    test_above = data_a[select_data]\n",
    "    \n",
    "    #test below\n",
    "    np.random.seed(0)\n",
    "    #slicing to get respective ratio of above to below UH data patches\n",
    "    select_data = np.random.permutation(data_b.shape[0])[int(data_a.shape[0]*split_perc):\n",
    "                                                         int((((data_a.shape[0]*(1-split_perc))*data_b.shape[0])/data_a.shape[0])+(data_a.shape[0]*(1-split_perc)))]\n",
    "    test_below = data_b[select_data]\n",
    "\n",
    "    #create the label data\n",
    "    train_above_label = np.ones(train_above.shape[0])\n",
    "    train_below_label = np.zeros(train_below.shape[0])\n",
    "    test_above_label = np.ones(test_above.shape[0])\n",
    "    test_below_label = np.zeros(test_below.shape[0])\n",
    "    \n",
    "    #merge above and below data in prep to shuffle/permute\n",
    "    train_data = np.vstack([train_above, train_below])\n",
    "    if return_label:\n",
    "        train_label = np.hstack([train_above_label, train_below_label])\n",
    "    test_data = np.vstack([test_above, test_below])\n",
    "    if return_label:\n",
    "        test_label = np.hstack([test_above_label, test_below_label])\n",
    "    \n",
    "    #finally, permute the data that has been merged and properly balanced\n",
    "    np.random.seed(10)\n",
    "    train_data = np.random.permutation(train_data)\n",
    "    np.random.seed(10)\n",
    "    test_data = np.random.permutation(test_data)\n",
    "    if not return_label:\n",
    "        return train_data, test_data  \n",
    "    if return_label:\n",
    "        np.random.seed(10)\n",
    "        train_label = np.random.permutation(train_label)\n",
    "        np.random.seed(10)\n",
    "        test_label = np.random.permutation(test_label)    \n",
    "        return train_data, test_data, train_label, test_label\n",
    "\n",
    "\n",
    "def minmax_scale_apply(thedata):\n",
    "    #apply min max normalize the input data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(thedata)\n",
    "    return scaler.transform(thedata)\n",
    "\n",
    "def standardize_scale_apply(thedata):\n",
    "    #standardization of the data\n",
    "    #to interpret: \"this data point is X standard deviations below/above the mean of the data set.\"\n",
    "    return np.divide((thedata - np.nanmean(thedata)), np.std(thedata))\n",
    "\n",
    "def standardize_scale_apply_test(thedatatrain, thedatatest):\n",
    "    #standardization of the test data using the training mean and standard deviation.\n",
    "    return np.divide((thedatatest - np.nanmean(thedatatrain)), np.std(thedatatrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the permuted below threshold data of the length of the above threshold data to balance the distribution of above and below threshold storm patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above \n",
    "mean_above_data_temp1 = data_above.temp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_temp3 = data_above.temp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_temp5 = data_above.temp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_temp7 = data_above.temp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_above_data_evwd1 = data_above.evwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_evwd3 = data_above.evwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_evwd5 = data_above.evwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_evwd7 = data_above.evwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_above_data_euwd1 = data_above.euwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_euwd3 = data_above.euwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_euwd5 = data_above.euwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_euwd7 = data_above.euwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_above_data_qvap1 = data_above.qvap_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qvap3 = data_above.qvap_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qvap5 = data_above.qvap_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qvap7 = data_above.qvap_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_above_data_pres1 = data_above.pres_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_pres3 = data_above.pres_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_pres5 = data_above.pres_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_pres7 = data_above.pres_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_above_data_qgrp1 = data_above.qgrp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qgrp3 = data_above.qgrp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qgrp5 = data_above.qgrp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_above_data_qgrp7 = data_above.qgrp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "#below\n",
    "mean_below_data_temp1 = data_below.temp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_temp3 = data_below.temp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_temp5 = data_below.temp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_temp7 = data_below.temp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_below_data_evwd1 = data_below.evwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_evwd3 = data_below.evwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_evwd5 = data_below.evwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_evwd7 = data_below.evwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_below_data_euwd1 = data_below.euwd_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_euwd3 = data_below.euwd_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_euwd5 = data_below.euwd_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_euwd7 = data_below.euwd_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_below_data_qvap1 = data_below.qvap_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qvap3 = data_below.qvap_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qvap5 = data_below.qvap_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qvap7 = data_below.qvap_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_below_data_pres1 = data_below.pres_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_pres3 = data_below.pres_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_pres5 = data_below.pres_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_pres7 = data_below.pres_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "\n",
    "mean_below_data_qgrp1 = data_below.qgrp_sev_1.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qgrp3 = data_below.qgrp_sev_3.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qgrp5 = data_below.qgrp_sev_5.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)\n",
    "mean_below_data_qgrp7 = data_below.qgrp_sev_7.mean(axis=(1,2), skipna=True).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_temp1, mean_test_temp1, train_label, test_label = create_traintest_data(mean_below_data_temp1, mean_above_data_temp1, split_perc=0.6, return_label=True)\n",
    "mean_train_temp3, mean_test_temp3 = create_traintest_data(mean_below_data_temp3, mean_above_data_temp3, split_perc=0.6, return_label=False)\n",
    "mean_train_temp5, mean_test_temp5 = create_traintest_data(mean_below_data_temp5, mean_above_data_temp5, split_perc=0.6, return_label=False)\n",
    "mean_train_temp7, mean_test_temp7 = create_traintest_data(mean_below_data_temp7, mean_above_data_temp7, split_perc=0.6, return_label=False)\n",
    "\n",
    "mean_train_evwd1, mean_test_evwd1 = create_traintest_data(mean_below_data_evwd1, mean_above_data_evwd1, split_perc=0.6, return_label=False)\n",
    "mean_train_evwd3, mean_test_evwd3 = create_traintest_data(mean_below_data_evwd3, mean_above_data_evwd3, split_perc=0.6, return_label=False)\n",
    "mean_train_evwd5, mean_test_evwd5 = create_traintest_data(mean_below_data_evwd5, mean_above_data_evwd5, split_perc=0.6, return_label=False)\n",
    "mean_train_evwd7, mean_test_evwd7 = create_traintest_data(mean_below_data_evwd7, mean_above_data_evwd7, split_perc=0.6, return_label=False)\n",
    "\n",
    "mean_train_euwd1, mean_test_euwd1 = create_traintest_data(mean_below_data_euwd1, mean_above_data_euwd1, split_perc=0.6, return_label=False)\n",
    "mean_train_euwd3, mean_test_euwd3 = create_traintest_data(mean_below_data_euwd3, mean_above_data_euwd3, split_perc=0.6, return_label=False)\n",
    "mean_train_euwd5, mean_test_euwd5 = create_traintest_data(mean_below_data_euwd5, mean_above_data_euwd5, split_perc=0.6, return_label=False)\n",
    "mean_train_euwd7, mean_test_euwd7 = create_traintest_data(mean_below_data_euwd7, mean_above_data_euwd7, split_perc=0.6, return_label=False)\n",
    "\n",
    "mean_train_qvap1, mean_test_qvap1 = create_traintest_data(mean_below_data_qvap1, mean_above_data_qvap1, split_perc=0.6, return_label=False)\n",
    "mean_train_qvap3, mean_test_qvap3 = create_traintest_data(mean_below_data_qvap3, mean_above_data_qvap3, split_perc=0.6, return_label=False)\n",
    "mean_train_qvap5, mean_test_qvap5 = create_traintest_data(mean_below_data_qvap5, mean_above_data_qvap5, split_perc=0.6, return_label=False)\n",
    "mean_train_qvap7, mean_test_qvap7 = create_traintest_data(mean_below_data_qvap7, mean_above_data_qvap7, split_perc=0.6, return_label=False)\n",
    "\n",
    "mean_train_pres1, mean_test_pres1 = create_traintest_data(mean_below_data_pres1, mean_above_data_pres1, split_perc=0.6, return_label=False)\n",
    "mean_train_pres3, mean_test_pres3 = create_traintest_data(mean_below_data_pres3, mean_above_data_pres3, split_perc=0.6, return_label=False)\n",
    "mean_train_pres5, mean_test_pres5 = create_traintest_data(mean_below_data_pres5, mean_above_data_pres5, split_perc=0.6, return_label=False)\n",
    "mean_train_pres7, mean_test_pres7 = create_traintest_data(mean_below_data_pres7, mean_above_data_pres7, split_perc=0.6, return_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_train_temp1 = standardize_scale_apply(mean_train_temp1)\n",
    "data_scaled_train_temp3 = standardize_scale_apply(mean_train_temp3)\n",
    "data_scaled_train_temp5 = standardize_scale_apply(mean_train_temp5)\n",
    "data_scaled_train_temp7 = standardize_scale_apply(mean_train_temp7)\n",
    "\n",
    "data_scaled_train_evwd1 = standardize_scale_apply(mean_train_evwd1)\n",
    "data_scaled_train_evwd3 = standardize_scale_apply(mean_train_evwd3)\n",
    "data_scaled_train_evwd5 = standardize_scale_apply(mean_train_evwd5)\n",
    "data_scaled_train_evwd7 = standardize_scale_apply(mean_train_evwd7)\n",
    "\n",
    "data_scaled_train_euwd1 = standardize_scale_apply(mean_train_euwd1)\n",
    "data_scaled_train_euwd3 = standardize_scale_apply(mean_train_euwd3)\n",
    "data_scaled_train_euwd5 = standardize_scale_apply(mean_train_euwd5)\n",
    "data_scaled_train_euwd7 = standardize_scale_apply(mean_train_euwd7)\n",
    "\n",
    "data_scaled_train_qvap1 = standardize_scale_apply(mean_train_qvap1)\n",
    "data_scaled_train_qvap3 = standardize_scale_apply(mean_train_qvap3)\n",
    "data_scaled_train_qvap5 = standardize_scale_apply(mean_train_qvap5)\n",
    "data_scaled_train_qvap7 = standardize_scale_apply(mean_train_qvap7)\n",
    "\n",
    "data_scaled_train_pres1 = standardize_scale_apply(mean_train_pres1)\n",
    "data_scaled_train_pres3 = standardize_scale_apply(mean_train_pres3)\n",
    "data_scaled_train_pres5 = standardize_scale_apply(mean_train_pres5)\n",
    "data_scaled_train_pres7 = standardize_scale_apply(mean_train_pres7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_test_temp1 = standardize_scale_apply_test(mean_train_temp1, mean_test_temp1)\n",
    "data_scaled_test_temp3 = standardize_scale_apply_test(mean_train_temp1, mean_test_temp3)\n",
    "data_scaled_test_temp5 = standardize_scale_apply_test(mean_train_temp1, mean_test_temp5)\n",
    "data_scaled_test_temp7 = standardize_scale_apply_test(mean_train_temp1, mean_test_temp7)\n",
    "\n",
    "data_scaled_test_evwd1 = standardize_scale_apply_test(mean_train_evwd1, mean_test_evwd1)\n",
    "data_scaled_test_evwd3 = standardize_scale_apply_test(mean_train_evwd3, mean_test_evwd3)\n",
    "data_scaled_test_evwd5 = standardize_scale_apply_test(mean_train_evwd5, mean_test_evwd5)\n",
    "data_scaled_test_evwd7 = standardize_scale_apply_test(mean_train_evwd7, mean_test_evwd7)\n",
    "\n",
    "data_scaled_test_euwd1 = standardize_scale_apply_test(mean_train_euwd1, mean_test_euwd1)\n",
    "data_scaled_test_euwd3 = standardize_scale_apply_test(mean_train_euwd3, mean_test_euwd3)\n",
    "data_scaled_test_euwd5 = standardize_scale_apply_test(mean_train_euwd5, mean_test_euwd5)\n",
    "data_scaled_test_euwd7 = standardize_scale_apply_test(mean_train_euwd7, mean_test_euwd7)\n",
    "\n",
    "data_scaled_test_qvap1 = standardize_scale_apply_test(mean_train_qvap1, mean_test_qvap1)\n",
    "data_scaled_test_qvap3 = standardize_scale_apply_test(mean_train_qvap3, mean_test_qvap3)\n",
    "data_scaled_test_qvap5 = standardize_scale_apply_test(mean_train_qvap5, mean_test_qvap5)\n",
    "data_scaled_test_qvap7 = standardize_scale_apply_test(mean_train_qvap7, mean_test_qvap7)\n",
    "\n",
    "data_scaled_test_pres1 = standardize_scale_apply_test(mean_train_pres1, mean_test_pres1)\n",
    "data_scaled_test_pres3 = standardize_scale_apply_test(mean_train_pres3, mean_test_pres3)\n",
    "data_scaled_test_pres5 = standardize_scale_apply_test(mean_train_pres5, mean_test_pres5)\n",
    "data_scaled_test_pres7 = standardize_scale_apply_test(mean_train_pres7, mean_test_pres7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([data_scaled_train_temp1, data_scaled_train_temp3, data_scaled_train_temp5, data_scaled_train_temp7,                     \n",
    "                     data_scaled_train_evwd1, data_scaled_train_evwd3, data_scaled_train_evwd5, data_scaled_train_evwd7,     \n",
    "                     data_scaled_train_euwd1, data_scaled_train_euwd3, data_scaled_train_euwd5, data_scaled_train_euwd7,\n",
    "                     data_scaled_train_qvap1, data_scaled_train_qvap3, data_scaled_train_qvap5, data_scaled_train_qvap7,\n",
    "                     data_scaled_train_pres1, data_scaled_train_pres3, data_scaled_train_pres5, data_scaled_train_pres7\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([data_scaled_test_temp1, data_scaled_test_temp3, data_scaled_test_temp5, data_scaled_test_temp7,\n",
    "                    data_scaled_test_evwd1, data_scaled_test_evwd3, data_scaled_test_evwd5, data_scaled_test_evwd7,\n",
    "                    data_scaled_test_euwd1, data_scaled_test_euwd3, data_scaled_test_euwd5, data_scaled_test_euwd7,\n",
    "                    data_scaled_test_qvap1, data_scaled_test_qvap3, data_scaled_test_qvap5, data_scaled_test_qvap7,\n",
    "                    data_scaled_test_pres1, data_scaled_test_pres3, data_scaled_test_pres5, data_scaled_test_pres7\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble = xr.Dataset({\n",
    "    'X_train':(['a','features'], X_train),\n",
    "    'X_train_label':(['a'], train_label),\n",
    "    'X_test':(['b','features'], X_test),\n",
    "    'X_test_label':(['b'], test_label),\n",
    "    },\n",
    "     coords=\n",
    "    {'feature':(['features'],np.array([\"tk_1km\", \"tk_3km\", \"tk_5km\", \"tk_7km\",\n",
    "                                       \"ev_1km\", \"ev_3km\", \"ev_5km\", \"ev_7km\",\n",
    "                                       \"eu_1km\", \"eu_3km\", \"eu_5km\", \"eu_7km\",\n",
    "                                       \"pr_1km\", \"pr_3km\", \"pr_5km\", \"pr_7km\",\n",
    "                                       \"qv_1km\", \"qv_3km\", \"qv_5km\", \"qv_7km\"])),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (a: 39940, b: 548339, features: 20)\n",
       "Coordinates:\n",
       "    feature        (features) <U6 'tk_1km' 'tk_3km' ... 'qv_5km' 'qv_7km'\n",
       "Dimensions without coordinates: a, b, features\n",
       "Data variables:\n",
       "    X_train        (a, features) float32 0.5012527 0.7719492 ... 0.17607747\n",
       "    X_train_label  (a) float64 1.0 1.0 1.0 0.0 1.0 1.0 ... 1.0 1.0 0.0 0.0 1.0\n",
       "    X_test         (b, features) float32 0.40386754 -1.7664597 ... -0.6877184\n",
       "    X_test_label   (b) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble.to_netcdf(f\"/glade/scratch/molina/WRF_CONUS1_derived/logistic_regression/{which_climate}_meanpatch_traintestdata_unbalanced.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
