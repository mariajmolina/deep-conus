{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Testing Data for Deep Learning \n",
    "## Version: Patch data during Current or Future Climate\n",
    "### Requires: create_UHindx_file_step1, ..file_step2, ..file_step3 (e.g., future_uh75patches_12.nc)\n",
    "\n",
    "First, import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the climate to work with (e.g., current or future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_climate = 'future'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dask workers with adaptive scaling to load data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q regular\n",
      "#PBS -A P54048000\n",
      "#PBS -l select=1:ncpus=36:mem=109GB\n",
      "#PBS -l walltime=01:00:00\n",
      "#PBS -e /glade/scratch/molina/\n",
      "#PBS -o /glade/scratch/molina/\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "\n",
      "\n",
      "/glade/work/molina/miniconda3/envs/python-tutorial/bin/python -m distributed.cli.dask_worker tcp://10.148.10.15:45223 --nthreads 36 --memory-limit 109.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60 --local-directory /glade/scratch/molina --interface ib0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.148.10.15:45223</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status' target='_blank'>https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.10.15:45223' processes=0 cores=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "\n",
    "#if __name__== \"__main__\":\n",
    "\n",
    "#start dask workers\n",
    "cluster = NCARCluster(memory=\"109GB\", cores=36)\n",
    "cluster.adapt(minimum=1, maximum=10, wait_count=60)\n",
    "cluster\n",
    "#print scripts\n",
    "print(cluster.job_script())\n",
    "#start client\n",
    "client = Client(cluster)\n",
    "client\n",
    "\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load storm patch data that was previously separated using UH>75 and UH<75 m2/s2 thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_12.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_jan_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_02.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_03.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_04.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_above = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_uh75patches_05.nc\", \n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_above = xr.concat([data_dec_above, data_jan_above, data_feb_above, data_mar_above, data_apr_above, data_may_above], dim='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_12.nc\",\n",
    "                                  parallel=True, combine='by_coords')\n",
    "data_jan_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_01.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_feb_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_02.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_mar_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_03.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_apr_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_04.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "data_may_below = xr.open_mfdataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/{which_climate}_nonuh75patches_05.nc\",\n",
    "                                   parallel=True, combine='by_coords')\n",
    "\n",
    "data_below = xr.concat([data_dec_below, data_jan_below, data_feb_below, data_mar_below, data_apr_below, data_may_below], dim='patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of strongly rotating to not strongly rotating storm patches expressed as a percent is about:  3 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The ratio of strongly rotating to not strongly rotating storm patches expressed as a percent is about: \", \n",
    "      round((data_above.patch.size/data_below.patch.size)*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of various functions for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traintest_data(data_b, data_a, split_perc=0.6, return_label=False):\n",
    "    #balancing of above and below threshold data for training data, spitting out remainder for testing\n",
    "    #permute and slice the below threshold data to equal the above threshold data shape.\n",
    "    \n",
    "    #train above\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_a.shape[0])[:int(data_a.shape[0]*split_perc)]\n",
    "    train_above = data_a[select_data]\n",
    "    \n",
    "    #train below\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_b.shape[0])[:int(data_a.shape[0]*split_perc)]\n",
    "    train_below = data_b[select_data]\n",
    "    \n",
    "    #test above\n",
    "    np.random.seed(0)\n",
    "    select_data = np.random.permutation(data_a.shape[0])[int(data_a.shape[0]*split_perc):]\n",
    "    test_above = data_a[select_data]\n",
    "    \n",
    "    #test below\n",
    "    np.random.seed(0)\n",
    "    #slicing to get respective ratio of above to below UH data patches\n",
    "    select_data = np.random.permutation(data_b.shape[0])[int(data_a.shape[0]*split_perc):\n",
    "                                                         int((((data_a.shape[0]*(1-split_perc))*data_b.shape[0])/data_a.shape[0])+(data_a.shape[0]*(1-split_perc)))]\n",
    "    test_below = data_b[select_data]\n",
    "\n",
    "    #create the label data\n",
    "    train_above_label = np.ones(train_above.shape[0])\n",
    "    train_below_label = np.zeros(train_below.shape[0])\n",
    "    test_above_label = np.ones(test_above.shape[0])\n",
    "    test_below_label = np.zeros(test_below.shape[0])\n",
    "    \n",
    "    #merge above and below data in prep to shuffle/permute\n",
    "    train_data = np.vstack([train_above, train_below])\n",
    "    if return_label:\n",
    "        train_label = np.hstack([train_above_label, train_below_label])\n",
    "    test_data = np.vstack([test_above, test_below])\n",
    "    if return_label:\n",
    "        test_label = np.hstack([test_above_label, test_below_label])\n",
    "    \n",
    "    #finally, permute the data that has been merged and properly balanced\n",
    "    np.random.seed(10)\n",
    "    train_data = np.random.permutation(train_data)\n",
    "    np.random.seed(10)\n",
    "    test_data = np.random.permutation(test_data)\n",
    "    if not return_label:\n",
    "        return train_data, test_data  \n",
    "    if return_label:\n",
    "        np.random.seed(10)\n",
    "        train_label = np.random.permutation(train_label)\n",
    "        np.random.seed(10)\n",
    "        test_label = np.random.permutation(test_label)    \n",
    "        return train_data, test_data, train_label, test_label\n",
    "\n",
    "\n",
    "def minmax_scale_apply(thedata):\n",
    "    #apply min max normalize the input data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(thedata)\n",
    "    return scaler.transform(thedata)\n",
    "\n",
    "def standardize_scale_apply(thedata):\n",
    "    #standardization of the data\n",
    "    #to interpret: \"this data point is X standard deviations below/above the mean of the data set.\"\n",
    "    return np.divide((thedata - np.nanmean(thedata)), np.nanstd(thedata))\n",
    "\n",
    "def standardize_scale_apply_test(thedatatrain, thedatatest):\n",
    "    #standardization of the test data using the training mean and standard deviation.\n",
    "    return np.divide((thedatatest - np.nanmean(thedatatrain)), np.nanstd(thedatatrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large size of the storm patch files (file generation triggered memory errors when writing deep learning files), data were preprocessed for deep learning models one variable at a time (across the four vertical levels). \n",
    "\n",
    "Selection of the variable occurs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "\n",
    "#T, EV, EU, QV, P\n",
    "\n",
    "what_analysis = 'P'\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if what_analysis == \"T\":\n",
    "    choice_var1 = \"temp_sev_1\"\n",
    "    choice_var3 = \"temp_sev_3\"\n",
    "    choice_var5 = \"temp_sev_5\"\n",
    "    choice_var7 = \"temp_sev_7\"\n",
    "    attrs_array = np.array([\"tk_1km\", \"tk_3km\", \"tk_5km\", \"tk_7km\"])\n",
    "    var_namer = \"tk\"\n",
    "\n",
    "if what_analysis == \"EV\":\n",
    "    choice_var1 = \"evwd_sev_1\"\n",
    "    choice_var3 = \"evwd_sev_3\"\n",
    "    choice_var5 = \"evwd_sev_5\"\n",
    "    choice_var7 = \"evwd_sev_7\"\n",
    "    attrs_array = np.array([\"ev_1km\", \"ev_3km\", \"ev_5km\", \"ev_7km\"])\n",
    "    var_namer = \"ev\"\n",
    "\n",
    "if what_analysis == \"EU\":\n",
    "    choice_var1 = \"euwd_sev_1\"\n",
    "    choice_var3 = \"euwd_sev_3\"\n",
    "    choice_var5 = \"euwd_sev_5\"\n",
    "    choice_var7 = \"euwd_sev_7\"\n",
    "    attrs_array = np.array([\"eu_1km\", \"eu_3km\", \"eu_5km\", \"eu_7km\"])\n",
    "    var_namer = \"eu\"\n",
    "\n",
    "if what_analysis == \"QV\":\n",
    "    choice_var1 = \"qvap_sev_1\"\n",
    "    choice_var3 = \"qvap_sev_3\"\n",
    "    choice_var5 = \"qvap_sev_5\"\n",
    "    choice_var7 = \"qvap_sev_7\"\n",
    "    attrs_array = np.array([\"qv_1km\", \"qv_3km\", \"qv_5km\", \"qv_7km\"])\n",
    "    var_namer = \"qv\"\n",
    "\n",
    "if what_analysis == \"P\":\n",
    "    choice_var1 = \"pres_sev_1\"\n",
    "    choice_var3 = \"pres_sev_3\"\n",
    "    choice_var5 = \"pres_sev_5\"\n",
    "    choice_var7 = \"pres_sev_7\"\n",
    "    attrs_array = np.array([\"pr_1km\", \"pr_3km\", \"pr_5km\", \"pr_7km\"])\n",
    "    var_namer = \"pr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the dask array data and load into numpy arrays, then permute/split data, and then standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above \n",
    "the_above_data_1 = data_above[choice_var1].values\n",
    "the_above_data_3 = data_above[choice_var3].values\n",
    "the_above_data_5 = data_above[choice_var5].values\n",
    "the_above_data_7 = data_above[choice_var7].values\n",
    "\n",
    "#below\n",
    "the_below_data_1 = data_below[choice_var1].values\n",
    "the_below_data_3 = data_below[choice_var3].values\n",
    "the_below_data_5 = data_below[choice_var5].values\n",
    "the_below_data_7 = data_below[choice_var7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_train_1, the_test_1, train_label, test_label = create_traintest_data(the_below_data_1, the_above_data_1, split_perc=0.6, return_label=True)\n",
    "the_train_3, the_test_3 = create_traintest_data(the_below_data_3, the_above_data_3, split_perc=0.6, return_label=False)\n",
    "the_train_5, the_test_5 = create_traintest_data(the_below_data_5, the_above_data_5, split_perc=0.6, return_label=False)\n",
    "the_train_7, the_test_7 = create_traintest_data(the_below_data_7, the_above_data_7, split_perc=0.6, return_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_train_1 = standardize_scale_apply(the_train_1)\n",
    "data_scaled_train_3 = standardize_scale_apply(the_train_3)\n",
    "data_scaled_train_5 = standardize_scale_apply(the_train_5)\n",
    "data_scaled_train_7 = standardize_scale_apply(the_train_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_test_1 = standardize_scale_apply_test(the_train_1, the_test_1)\n",
    "data_scaled_test_3 = standardize_scale_apply_test(the_train_3, the_test_3)\n",
    "data_scaled_test_5 = standardize_scale_apply_test(the_train_5, the_test_5)\n",
    "data_scaled_test_7 = standardize_scale_apply_test(the_train_7, the_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack([data_scaled_train_1, data_scaled_train_3, data_scaled_train_5, data_scaled_train_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.stack([data_scaled_test_1, data_scaled_test_3, data_scaled_test_5, data_scaled_test_7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create xarray dataset for saving for the specific variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble = xr.Dataset({\n",
    "    'X_train':(['a','x','y','features'], X_train.reshape(X_train.shape[1],32,32,4)),\n",
    "    'X_train_label':(['a'], train_label),\n",
    "    'X_test':(['b','x','y','features'], X_test.reshape(X_test.shape[1],32,32,4)),\n",
    "    'X_test_label':(['b'], test_label),\n",
    "    },\n",
    "     coords=\n",
    "    {'feature':(['features'],attrs_array),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (a: 47702, b: 560022, features: 4, x: 32, y: 32)\n",
       "Coordinates:\n",
       "    feature        (features) <U6 'pr_1km' 'pr_3km' 'pr_5km' 'pr_7km'\n",
       "Dimensions without coordinates: a, b, features, x, y\n",
       "Data variables:\n",
       "    X_train        (a, x, y, features) float32 1.019782 1.0193924 ... -1.871275\n",
       "    X_train_label  (a) float64 1.0 1.0 0.0 0.0 1.0 1.0 ... 0.0 0.0 0.0 0.0 1.0\n",
       "    X_test         (b, x, y, features) float32 0.528231 0.52410966 ... 0.7838981\n",
       "    X_test_label   (b) float64 0.0 0.0 0.0 0.0 0.0 1.0 ... 0.0 0.0 0.0 0.0 0.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble.to_netcdf(f\"/glade/scratch/molina/WRF_CONUS1_derived/deep_learning/{which_climate}_{var_namer}patch_traintestdata_unbalanced.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
