{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Testing Data for a Logistic Regression \n",
    "## Version: Using Max UH location data during Current and Future Climate (UNBALANCED)\n",
    "\n",
    "First, import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dask workers with adaptive scaling to load data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q regular\n",
      "#PBS -A P54048000\n",
      "#PBS -l select=1:ncpus=36:mem=109GB\n",
      "#PBS -l walltime=01:00:00\n",
      "#PBS -e /glade/scratch/molina/\n",
      "#PBS -o /glade/scratch/molina/\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "\n",
      "\n",
      "/glade/work/molina/miniconda3/envs/python-tutorial/bin/python -m distributed.cli.dask_worker tcp://10.148.10.17:42307 --nthreads 36 --memory-limit 109.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60 --local-directory /glade/scratch/molina --interface ib0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.148.10.17:42307</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status' target='_blank'>https://jupyterhub.ucar.edu/ch/user/molina/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.10.17:42307' processes=0 cores=0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "\n",
    "#if __name__== \"__main__\":\n",
    "\n",
    "#start dask workers\n",
    "cluster = NCARCluster(memory=\"109GB\", cores=36)\n",
    "cluster.adapt(minimum=1, maximum=10, wait_count=60)\n",
    "cluster\n",
    "#print scripts\n",
    "print(cluster.job_script())\n",
    "#start client\n",
    "client = Client(cluster)\n",
    "client\n",
    "\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open data sets that contain variables at maximum UH locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_current = xr.open_dataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/current_conus1_varsatmaxUH.nc\")\n",
    "data_futures = xr.open_dataset(f\"/glade/scratch/molina/WRF_CONUS1_derived/storm_envs/future_conus1_varsatmaxUH.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print relevant shapes of current and future climate data. Future climate has more storm patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of storm patches in current climate:  1387488\n",
      "Total number of storm patches in future climate:  1419928\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of storm patches in current climate: \",data_current.x.shape[0])\n",
    "print(\"Total number of storm patches in future climate: \",data_futures.x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate storm patch data into above and below UH 75 m2/s2 threshold groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uh0_above_current = data_current.data_prd[data_current.data_prd>=75]\n",
    "\n",
    "data_tk1_above_current = data_current.data_var_tk1[data_current.data_prd>=75]\n",
    "data_tk3_above_current = data_current.data_var_tk3[data_current.data_prd>=75]\n",
    "data_tk5_above_current = data_current.data_var_tk5[data_current.data_prd>=75]\n",
    "data_tk7_above_current = data_current.data_var_tk7[data_current.data_prd>=75]\n",
    "\n",
    "data_ev1_above_current = data_current.data_var_ev1[data_current.data_prd>=75]\n",
    "data_ev3_above_current = data_current.data_var_ev3[data_current.data_prd>=75]\n",
    "data_ev5_above_current = data_current.data_var_ev5[data_current.data_prd>=75]\n",
    "data_ev7_above_current = data_current.data_var_ev7[data_current.data_prd>=75]\n",
    "\n",
    "data_eu1_above_current = data_current.data_var_eu1[data_current.data_prd>=75]\n",
    "data_eu3_above_current = data_current.data_var_eu3[data_current.data_prd>=75]\n",
    "data_eu5_above_current = data_current.data_var_eu5[data_current.data_prd>=75]\n",
    "data_eu7_above_current = data_current.data_var_eu7[data_current.data_prd>=75]\n",
    "\n",
    "data_pr1_above_current = data_current.data_var_pr1[data_current.data_prd>=75]\n",
    "data_pr3_above_current = data_current.data_var_pr3[data_current.data_prd>=75]\n",
    "data_pr5_above_current = data_current.data_var_pr5[data_current.data_prd>=75]\n",
    "data_pr7_above_current = data_current.data_var_pr7[data_current.data_prd>=75]\n",
    "\n",
    "data_qv1_above_current = data_current.data_var_qv1[data_current.data_prd>=75]\n",
    "data_qv3_above_current = data_current.data_var_qv3[data_current.data_prd>=75]\n",
    "data_qv5_above_current = data_current.data_var_qv5[data_current.data_prd>=75]\n",
    "data_qv7_above_current = data_current.data_var_qv7[data_current.data_prd>=75]\n",
    "\n",
    "data_uh0_below_current = data_current.data_prd[data_current.data_prd<75]\n",
    "\n",
    "data_tk1_below_current = data_current.data_var_tk1[data_current.data_prd<75]\n",
    "data_tk3_below_current = data_current.data_var_tk3[data_current.data_prd<75]\n",
    "data_tk5_below_current = data_current.data_var_tk5[data_current.data_prd<75]\n",
    "data_tk7_below_current = data_current.data_var_tk7[data_current.data_prd<75]\n",
    "\n",
    "data_ev1_below_current = data_current.data_var_ev1[data_current.data_prd<75]\n",
    "data_ev3_below_current = data_current.data_var_ev3[data_current.data_prd<75]\n",
    "data_ev5_below_current = data_current.data_var_ev5[data_current.data_prd<75]\n",
    "data_ev7_below_current = data_current.data_var_ev7[data_current.data_prd<75]\n",
    "\n",
    "data_eu1_below_current = data_current.data_var_eu1[data_current.data_prd<75]\n",
    "data_eu3_below_current = data_current.data_var_eu3[data_current.data_prd<75]\n",
    "data_eu5_below_current = data_current.data_var_eu5[data_current.data_prd<75]\n",
    "data_eu7_below_current = data_current.data_var_eu7[data_current.data_prd<75]\n",
    "\n",
    "data_pr1_below_current = data_current.data_var_pr1[data_current.data_prd<75]\n",
    "data_pr3_below_current = data_current.data_var_pr3[data_current.data_prd<75]\n",
    "data_pr5_below_current = data_current.data_var_pr5[data_current.data_prd<75]\n",
    "data_pr7_below_current = data_current.data_var_pr7[data_current.data_prd<75]\n",
    "\n",
    "data_qv1_below_current = data_current.data_var_qv1[data_current.data_prd<75]\n",
    "data_qv3_below_current = data_current.data_var_qv3[data_current.data_prd<75]\n",
    "data_qv5_below_current = data_current.data_var_qv5[data_current.data_prd<75]\n",
    "data_qv7_below_current = data_current.data_var_qv7[data_current.data_prd<75]\n",
    "\n",
    "data_uh0_above_futures = data_futures.data_prd[data_futures.data_prd>=75]\n",
    "\n",
    "data_tk1_above_futures = data_futures.data_var_tk1[data_futures.data_prd>=75]\n",
    "data_tk3_above_futures = data_futures.data_var_tk3[data_futures.data_prd>=75]\n",
    "data_tk5_above_futures = data_futures.data_var_tk5[data_futures.data_prd>=75]\n",
    "data_tk7_above_futures = data_futures.data_var_tk7[data_futures.data_prd>=75]\n",
    "\n",
    "data_ev1_above_futures = data_futures.data_var_ev1[data_futures.data_prd>=75]\n",
    "data_ev3_above_futures = data_futures.data_var_ev3[data_futures.data_prd>=75]\n",
    "data_ev5_above_futures = data_futures.data_var_ev5[data_futures.data_prd>=75]\n",
    "data_ev7_above_futures = data_futures.data_var_ev7[data_futures.data_prd>=75]\n",
    "\n",
    "data_eu1_above_futures = data_futures.data_var_eu1[data_futures.data_prd>=75]\n",
    "data_eu3_above_futures = data_futures.data_var_eu3[data_futures.data_prd>=75]\n",
    "data_eu5_above_futures = data_futures.data_var_eu5[data_futures.data_prd>=75]\n",
    "data_eu7_above_futures = data_futures.data_var_eu7[data_futures.data_prd>=75]\n",
    "\n",
    "data_pr1_above_futures = data_futures.data_var_pr1[data_futures.data_prd>=75]\n",
    "data_pr3_above_futures = data_futures.data_var_pr3[data_futures.data_prd>=75]\n",
    "data_pr5_above_futures = data_futures.data_var_pr5[data_futures.data_prd>=75]\n",
    "data_pr7_above_futures = data_futures.data_var_pr7[data_futures.data_prd>=75]\n",
    "\n",
    "data_qv1_above_futures = data_futures.data_var_qv1[data_futures.data_prd>=75]\n",
    "data_qv3_above_futures = data_futures.data_var_qv3[data_futures.data_prd>=75]\n",
    "data_qv5_above_futures = data_futures.data_var_qv5[data_futures.data_prd>=75]\n",
    "data_qv7_above_futures = data_futures.data_var_qv7[data_futures.data_prd>=75]\n",
    "\n",
    "data_uh0_below_futures = data_futures.data_prd[data_futures.data_prd<75]\n",
    "\n",
    "data_tk1_below_futures = data_futures.data_var_tk1[data_futures.data_prd<75]\n",
    "data_tk3_below_futures = data_futures.data_var_tk3[data_futures.data_prd<75]\n",
    "data_tk5_below_futures = data_futures.data_var_tk5[data_futures.data_prd<75]\n",
    "data_tk7_below_futures = data_futures.data_var_tk7[data_futures.data_prd<75]\n",
    "\n",
    "data_ev1_below_futures = data_futures.data_var_ev1[data_futures.data_prd<75]\n",
    "data_ev3_below_futures = data_futures.data_var_ev3[data_futures.data_prd<75]\n",
    "data_ev5_below_futures = data_futures.data_var_ev5[data_futures.data_prd<75]\n",
    "data_ev7_below_futures = data_futures.data_var_ev7[data_futures.data_prd<75]\n",
    "\n",
    "data_eu1_below_futures = data_futures.data_var_eu1[data_futures.data_prd<75]\n",
    "data_eu3_below_futures = data_futures.data_var_eu3[data_futures.data_prd<75]\n",
    "data_eu5_below_futures = data_futures.data_var_eu5[data_futures.data_prd<75]\n",
    "data_eu7_below_futures = data_futures.data_var_eu7[data_futures.data_prd<75]\n",
    "\n",
    "data_pr1_below_futures = data_futures.data_var_pr1[data_futures.data_prd<75]\n",
    "data_pr3_below_futures = data_futures.data_var_pr3[data_futures.data_prd<75]\n",
    "data_pr5_below_futures = data_futures.data_var_pr5[data_futures.data_prd<75]\n",
    "data_pr7_below_futures = data_futures.data_var_pr7[data_futures.data_prd<75]\n",
    "\n",
    "data_qv1_below_futures = data_futures.data_var_qv1[data_futures.data_prd<75]\n",
    "data_qv3_below_futures = data_futures.data_var_qv3[data_futures.data_prd<75]\n",
    "data_qv5_below_futures = data_futures.data_var_qv5[data_futures.data_prd<75]\n",
    "data_qv7_below_futures = data_futures.data_var_qv7[data_futures.data_prd<75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more storm patches that exceed the UH threshold in the future climate than the current climate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current climate >75m2/s2:  33284\n",
      "Future climate >75m2/s2:  39753\n"
     ]
    }
   ],
   "source": [
    "print(\"Current climate >75m2/s2: \",data_uh0_above_current.shape[0])\n",
    "print(\"Future climate >75m2/s2: \",data_uh0_above_futures.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of various functions for use in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_apply(thedata):\n",
    "    #apply min max normalize the input data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(thedata)\n",
    "    return scaler.transform(thedata)\n",
    "\n",
    "def standardize_scale_apply(thedata):\n",
    "    #standardization of the data\n",
    "    #to interpret: \"this data point is X standard deviations below/above the mean of the data set.\"\n",
    "    return np.divide((thedata - np.nanmean(thedata)), np.std(thedata))\n",
    "\n",
    "def standardize_scale_apply_test(thedatatrain, thedatatest):\n",
    "    #standardization of the test data using the training mean and standard deviation.\n",
    "    return np.divide((thedatatest - np.nanmean(thedatatrain)), np.std(thedatatrain))\n",
    "\n",
    "\n",
    "def data_permute(data_a, data_b, data_split=0.6, spit_result=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function splits the data into desired percentage of training versus test. \n",
    "    Data is permuted (shuffled) prior to being split.\n",
    "    \n",
    "    Input Parameters:\n",
    "    data_a: data above the specified UH threshold (np.array).\n",
    "    data_b: data below the specified UH threshold (np.array).\n",
    "    data_split: percent of total data to be used for training (float).\n",
    "    split_result: whether to split and permute the label data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #split and permute the above threshold data.\n",
    "    np.random.seed(0)\n",
    "    select_train_a = np.random.permutation(data_a.shape[0])[:int(data_a.shape[0]*data_split)]\n",
    "    np.random.seed(0)\n",
    "    select_test_a = np.random.permutation(data_a.shape[0])[int(data_a.shape[0]*data_split):]\n",
    "    train_patches_a = data_a[select_train_a]\n",
    "    test_patches_a = data_a[select_test_a]\n",
    "    \n",
    "    #split and permute the below threshold data using above threshold total data shape.\n",
    "    np.random.seed(0)\n",
    "    select_train_b = np.random.permutation(data_b.shape[0])[:int(data_a.shape[0]*data_split)]\n",
    "    np.random.seed(0)\n",
    "    select_test_b = np.random.permutation(data_b.shape[0])[int(data_a.shape[0]*data_split):\n",
    "                                                           int((((data_a.shape[0]*(1-data_split))*data_b.shape[0])/data_a.shape[0])+(data_a.shape[0]*(1-data_split)))]\n",
    "    train_patches_b = data_b[select_train_b]\n",
    "    test_patches_b = data_b[select_test_b]\n",
    "    \n",
    "    #combine the above and below threshold data into one total training data set.\n",
    "    total_train_data = np.hstack([train_patches_a, train_patches_b])\n",
    "    if spit_result:\n",
    "        result_train_data = np.hstack([np.ones(train_patches_a.shape[0]), np.zeros(train_patches_b.shape[0])])\n",
    "    #combine the above and below test data into one total test data set.\n",
    "    total_test_data = np.hstack([test_patches_a, test_patches_b])\n",
    "    if spit_result:\n",
    "        result_test_data = np.hstack([np.ones(test_patches_a.shape[0]), np.zeros(test_patches_b.shape[0])])\n",
    "    \n",
    "    #shuffle/permute the combined dataset.\n",
    "    np.random.seed(5)\n",
    "    indx_1 = np.random.permutation(total_train_data.shape[0])\n",
    "    indx_1 = total_train_data[indx_1]\n",
    "    if spit_result:\n",
    "        np.random.seed(5)\n",
    "        indx_2 = np.random.permutation(result_train_data.shape[0])\n",
    "        indx_2 = result_train_data[indx_2]\n",
    "    #shuffle/permute the combined dataset.\n",
    "    np.random.seed(10)\n",
    "    indx_3 = np.random.permutation(total_test_data.shape[0])\n",
    "    indx_3 = total_test_data[indx_3]\n",
    "    if spit_result:\n",
    "        np.random.seed(10)\n",
    "        indx_4 = np.random.permutation(result_test_data.shape[0])\n",
    "        indx_4 = result_test_data[indx_4]\n",
    "        \n",
    "    #return data.\n",
    "    if not spit_result:\n",
    "        return indx_1, indx_3\n",
    "    if spit_result:\n",
    "        return indx_1, indx_3, indx_2, indx_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permute, combine, and split the above and below threshold data into training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tk1_current, test_tk1_current, train_uh0_current, test_uh0_current = data_permute(data_tk1_above_current, \n",
    "                                                                                        data_tk1_below_current, spit_result=True)\n",
    "\n",
    "train_tk3_current, test_tk3_current = data_permute(data_tk3_above_current, data_tk3_below_current)\n",
    "train_tk5_current, test_tk5_current = data_permute(data_tk5_above_current, data_tk5_below_current)\n",
    "train_tk7_current, test_tk7_current = data_permute(data_tk7_above_current, data_tk7_below_current)\n",
    "\n",
    "train_ev1_current, test_ev1_current = data_permute(data_ev1_above_current, data_ev1_below_current)\n",
    "train_ev3_current, test_ev3_current = data_permute(data_ev3_above_current, data_ev3_below_current)\n",
    "train_ev5_current, test_ev5_current = data_permute(data_ev5_above_current, data_ev5_below_current)\n",
    "train_ev7_current, test_ev7_current = data_permute(data_ev7_above_current, data_ev7_below_current)\n",
    "\n",
    "train_eu1_current, test_eu1_current = data_permute(data_eu1_above_current, data_eu1_below_current)\n",
    "train_eu3_current, test_eu3_current = data_permute(data_eu3_above_current, data_eu3_below_current)\n",
    "train_eu5_current, test_eu5_current = data_permute(data_eu5_above_current, data_eu5_below_current)\n",
    "train_eu7_current, test_eu7_current = data_permute(data_eu7_above_current, data_eu7_below_current)\n",
    "\n",
    "train_pr1_current, test_pr1_current = data_permute(data_pr1_above_current, data_pr1_below_current)\n",
    "train_pr3_current, test_pr3_current = data_permute(data_pr3_above_current, data_pr3_below_current)\n",
    "train_pr5_current, test_pr5_current = data_permute(data_pr5_above_current, data_pr5_below_current)\n",
    "train_pr7_current, test_pr7_current = data_permute(data_pr7_above_current, data_pr7_below_current)\n",
    "\n",
    "train_qv1_current, test_qv1_current = data_permute(data_qv1_above_current, data_qv1_below_current)\n",
    "train_qv3_current, test_qv3_current = data_permute(data_qv3_above_current, data_qv3_below_current)\n",
    "train_qv5_current, test_qv5_current = data_permute(data_qv5_above_current, data_qv5_below_current)\n",
    "train_qv7_current, test_qv7_current = data_permute(data_qv7_above_current, data_qv7_below_current)\n",
    "\n",
    "\n",
    "train_tk1_futures, test_tk1_futures, train_uh0_futures, test_uh0_futures = data_permute(data_tk1_above_futures, \n",
    "                                                                                        data_tk1_below_futures, spit_result=True)\n",
    "\n",
    "train_tk3_futures, test_tk3_futures = data_permute(data_tk3_above_futures, data_tk3_below_futures)\n",
    "train_tk5_futures, test_tk5_futures = data_permute(data_tk5_above_futures, data_tk5_below_futures)\n",
    "train_tk7_futures, test_tk7_futures = data_permute(data_tk7_above_futures, data_tk7_below_futures)\n",
    "\n",
    "train_ev1_futures, test_ev1_futures = data_permute(data_ev1_above_futures, data_ev1_below_futures)\n",
    "train_ev3_futures, test_ev3_futures = data_permute(data_ev3_above_futures, data_ev3_below_futures)\n",
    "train_ev5_futures, test_ev5_futures = data_permute(data_ev5_above_futures, data_ev5_below_futures)\n",
    "train_ev7_futures, test_ev7_futures = data_permute(data_ev7_above_futures, data_ev7_below_futures)\n",
    "\n",
    "train_eu1_futures, test_eu1_futures = data_permute(data_eu1_above_futures, data_eu1_below_futures)\n",
    "train_eu3_futures, test_eu3_futures = data_permute(data_eu3_above_futures, data_eu3_below_futures)\n",
    "train_eu5_futures, test_eu5_futures = data_permute(data_eu5_above_futures, data_eu5_below_futures)\n",
    "train_eu7_futures, test_eu7_futures = data_permute(data_eu7_above_futures, data_eu7_below_futures)\n",
    "\n",
    "train_pr1_futures, test_pr1_futures = data_permute(data_pr1_above_futures, data_pr1_below_futures)\n",
    "train_pr3_futures, test_pr3_futures = data_permute(data_pr3_above_futures, data_pr3_below_futures)\n",
    "train_pr5_futures, test_pr5_futures = data_permute(data_pr5_above_futures, data_pr5_below_futures)\n",
    "train_pr7_futures, test_pr7_futures = data_permute(data_pr7_above_futures, data_pr7_below_futures)\n",
    "\n",
    "train_qv1_futures, test_qv1_futures = data_permute(data_qv1_above_futures, data_qv1_below_futures)\n",
    "train_qv3_futures, test_qv3_futures = data_permute(data_qv3_above_futures, data_qv3_below_futures)\n",
    "train_qv5_futures, test_qv5_futures = data_permute(data_qv5_above_futures, data_qv5_below_futures)\n",
    "train_qv7_futures, test_qv7_futures = data_permute(data_qv7_above_futures, data_qv7_below_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array containing the training features in correct shape, with standardization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_current = np.hstack([\n",
    "    standardize_scale_apply(train_tk1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk7_current.reshape(-1,1)),                     \n",
    "    \n",
    "    standardize_scale_apply(train_ev1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev7_current.reshape(-1,1)),   \n",
    "    \n",
    "    standardize_scale_apply(train_eu1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu7_current.reshape(-1,1)),                     \n",
    "                     \n",
    "    standardize_scale_apply(train_pr1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr7_current.reshape(-1,1)),                     \n",
    "                     \n",
    "    standardize_scale_apply(train_qv1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv7_current.reshape(-1,1))\n",
    "])\n",
    "\n",
    "X_test_current = np.hstack([\n",
    "    standardize_scale_apply_test(train_tk1_current.reshape(-1,1), test_tk1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk3_current.reshape(-1,1), test_tk3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk5_current.reshape(-1,1), test_tk5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk7_current.reshape(-1,1), test_tk7_current.reshape(-1,1)), \n",
    "    \n",
    "    standardize_scale_apply_test(train_ev1_current.reshape(-1,1), test_ev1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev3_current.reshape(-1,1), test_ev3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev5_current.reshape(-1,1), test_ev5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev7_current.reshape(-1,1), test_ev7_current.reshape(-1,1)),    \n",
    "    \n",
    "    standardize_scale_apply_test(train_eu1_current.reshape(-1,1), test_eu1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu3_current.reshape(-1,1), test_eu3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu5_current.reshape(-1,1), test_eu5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu7_current.reshape(-1,1), test_eu7_current.reshape(-1,1)), \n",
    "    \n",
    "    standardize_scale_apply_test(train_pr1_current.reshape(-1,1), test_pr1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr3_current.reshape(-1,1), test_pr3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr5_current.reshape(-1,1), test_pr5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr7_current.reshape(-1,1), test_pr7_current.reshape(-1,1)),  \n",
    "    \n",
    "    standardize_scale_apply_test(train_qv1_current.reshape(-1,1), test_qv1_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv3_current.reshape(-1,1), test_qv3_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv5_current.reshape(-1,1), test_qv5_current.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv7_current.reshape(-1,1), test_qv7_current.reshape(-1,1))\n",
    "])\n",
    "\n",
    "X_train_futures = np.hstack([\n",
    "    standardize_scale_apply(train_tk1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_tk7_futures.reshape(-1,1)),          \n",
    "                                          \n",
    "    standardize_scale_apply(train_ev1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_ev7_futures.reshape(-1,1)),                     \n",
    "                     \n",
    "    standardize_scale_apply(train_eu1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_eu7_futures.reshape(-1,1)),                     \n",
    "                     \n",
    "    standardize_scale_apply(train_pr1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_pr7_futures.reshape(-1,1)),                     \n",
    "                     \n",
    "    standardize_scale_apply(train_qv1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply(train_qv7_futures.reshape(-1,1))\n",
    "])\n",
    "\n",
    "X_test_futures = np.hstack([\n",
    "    standardize_scale_apply_test(train_tk1_futures.reshape(-1,1), test_tk1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk3_futures.reshape(-1,1), test_tk3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk5_futures.reshape(-1,1), test_tk5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_tk7_futures.reshape(-1,1), test_tk7_futures.reshape(-1,1)), \n",
    "    \n",
    "    standardize_scale_apply_test(train_ev1_futures.reshape(-1,1), test_ev1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev3_futures.reshape(-1,1), test_ev3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev5_futures.reshape(-1,1), test_ev5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_ev7_futures.reshape(-1,1), test_ev7_futures.reshape(-1,1)),    \n",
    "    \n",
    "    standardize_scale_apply_test(train_eu1_futures.reshape(-1,1), test_eu1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu3_futures.reshape(-1,1), test_eu3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu5_futures.reshape(-1,1), test_eu5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_eu7_futures.reshape(-1,1), test_eu7_futures.reshape(-1,1)), \n",
    "    \n",
    "    standardize_scale_apply_test(train_pr1_futures.reshape(-1,1), test_pr1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr3_futures.reshape(-1,1), test_pr3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr5_futures.reshape(-1,1), test_pr5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_pr7_futures.reshape(-1,1), test_pr7_futures.reshape(-1,1)),  \n",
    "    \n",
    "    standardize_scale_apply_test(train_qv1_futures.reshape(-1,1), test_qv1_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv3_futures.reshape(-1,1), test_qv3_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv5_futures.reshape(-1,1), test_qv5_futures.reshape(-1,1)), \n",
    "    standardize_scale_apply_test(train_qv7_futures.reshape(-1,1), test_qv7_futures.reshape(-1,1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39940, 20)\n",
      "(548339, 20)\n",
      "(47702, 20)\n",
      "(560022, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_current.shape)\n",
    "print(X_test_current.shape)\n",
    "print(X_train_futures.shape)\n",
    "print(X_test_futures.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training and testing data for current and future climate as one file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble = xr.Dataset({\n",
    "    'X_train_current':(['a','features'], X_train_current),\n",
    "    'X_train_current_label':(['a'], train_uh0_current),\n",
    "    'X_test_current':(['b','features'], X_test_current),\n",
    "    'X_test_current_label':(['b'], test_uh0_current),\n",
    "    'X_train_futures':(['c','features'], X_train_futures),\n",
    "    'X_train_futures_label':(['c'], train_uh0_futures),\n",
    "    'X_test_futures':(['d','features'], X_test_futures),\n",
    "    'X_test_futures_label':(['d'], test_uh0_futures),\n",
    "    },\n",
    "     coords=\n",
    "    {'feature':(['features'],np.array([\"tk_1km\", \"tk_3km\", \"tk_5km\", \"tk_7km\",\n",
    "                                       \"ev_1km\", \"ev_3km\", \"ev_5km\", \"ev_7km\",\n",
    "                                       \"eu_1km\", \"eu_3km\", \"eu_5km\", \"eu_7km\",\n",
    "                                       \"pr_1km\", \"pr_3km\", \"pr_5km\", \"pr_7km\",\n",
    "                                       \"qv_1km\", \"qv_3km\", \"qv_5km\", \"qv_7km\"])),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                (a: 39940, b: 548339, c: 47702, d: 560022, features: 20)\n",
       "Coordinates:\n",
       "    feature                (features) <U6 'tk_1km' 'tk_3km' ... 'qv_7km'\n",
       "Dimensions without coordinates: a, b, c, d, features\n",
       "Data variables:\n",
       "    X_train_current        (a, features) float32 0.916849 ... -1.7484128\n",
       "    X_train_current_label  (a) float64 1.0 1.0 1.0 0.0 1.0 ... 0.0 0.0 1.0 0.0\n",
       "    X_test_current         (b, features) float32 -0.4930225 ... -1.129027\n",
       "    X_test_current_label   (b) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
       "    X_train_futures        (c, features) float32 -0.8571867 ... 2.1425257\n",
       "    X_train_futures_label  (c) float64 1.0 0.0 1.0 0.0 1.0 ... 0.0 1.0 1.0 0.0\n",
       "    X_test_futures         (d, features) float32 0.73516613 ... -0.760373\n",
       "    X_test_futures_label   (d) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assemble.to_netcdf(\"/glade/scratch/molina/WRF_CONUS1_derived/logistic_regression/varsatUHmax_traintestdata_unbalanced.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
